#!/usr/bin/env python
import optparse
import sys
import models
from collections import defaultdict

optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="data/input", help="File containing sentences to translate (default=data/input)")
optparser.add_option("-t", "--translation-model", dest="tm", default="data/tm", help="File containing translation model (default=data/tm)")
optparser.add_option("-l", "--language-model", dest="lm", default="data/lm", help="File containing ARPA-format language model (default=data/lm)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxsize, type="int", help="Number of sentences to decode (default=no limit)")
optparser.add_option("-k", "--translations-per-phrase", dest="k", default=5, type="int", help="Limit on number of translations to consider per phrase (default=1)")
optparser.add_option("-s", "--stack-size", dest="s", default=1000, type="int", help="Maximum stack size (default=1)")
optparser.add_option("--lambda", dest="lambda_", default=0.5, type=float, help="Lagrange multiplier for phrase usage penalty (default=0.5)")
optparser.add_option("--distortion", dest="distortion_weight", default=0.3, type=float, help="Weight for distortion penalty (default=0.3)")
optparser.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False,  help="Verbose mode (default=off)")
opts = optparser.parse_args()[0]

tm = models.TM(opts.tm, opts.k)
lm = models.LM(opts.lm)
french_sentences = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

# Translate unknown words as-is with probability 1
for word in set(sum(french_sentences, ())):
    if (word,) not in tm:
        # sys.stderr.write(f"WARNING: '{word}' not in translation model!\n")
        tm[(word,)] = [models.phrase(word, 0.0)]

sys.stderr.write(f"Decoding {opts.input}...\n")
unaligned_count = 0  # Track unaligned sentences

class Hypothesis:
    def __init__(self, logprob, lm_state, predecessor, phrase, coverage, last_end, phrase_count):
        self.logprob = logprob
        self.lm_state = lm_state
        self.predecessor = predecessor
        self.phrase = phrase
        self.coverage = coverage  # Bitset representing covered positions
        self.last_end = last_end  # End position of the last phrase in the source sentence
        self.phrase_count = phrase_count  # Dictionary mapping phrases to counts

for f in french_sentences:
    initial_coverage = frozenset()
    initial_hypothesis = Hypothesis(0.0, lm.begin(), None, None, initial_coverage, 0, defaultdict(int))
    stacks = [{} for _ in range(len(f) + 1)]
    stacks[0][(lm.begin(), initial_coverage)] = initial_hypothesis

    for stack_num in range(len(f)):
        if not stacks[stack_num]:
            continue

        # Prune the current stack to the top 's' hypotheses
        current_stack = sorted(stacks[stack_num].values(), key=lambda h: -h.logprob)[:opts.s]
        stacks[stack_num] = { (h.lm_state, h.coverage): h for h in current_stack }

        for h in stacks[stack_num].values():
            for i in range(len(f)):
                if i in h.coverage:
                    continue  # Skip already covered positions
                for j in range(i + 1, len(f) + 1):
                    if all(k not in h.coverage for k in range(i, j)) and f[i:j] in tm:
                        for phrase in tm[f[i:j]]:
                            # Copy the phrase count from the predecessor and update
                            phrase_count = h.phrase_count.copy()
                            phrase_count[phrase.english] += 1

                            # Compute phrase usage penalty
                            penalty = opts.lambda_ * max(phrase_count[phrase.english] - opts.k, 0)

                            # Compute distortion cost
                            if h.phrase is None:
                                distortion = 0
                            else:
                                distortion = abs(i - h.last_end)
                            distortion_cost = opts.distortion_weight * distortion

                            logprob = h.logprob + phrase.logprob - penalty - distortion_cost
                            lm_state = h.lm_state
                            for word in phrase.english.split():
                                (lm_state, word_logprob) = lm.score(lm_state, word)
                                logprob += word_logprob
                            # No need to add lm.end(lm_state) here because we handle it after building the full hypothesis

                            # Update coverage
                            new_coverage = h.coverage.union(range(i, j))

                            new_hypothesis = Hypothesis(logprob, lm_state, h, phrase, new_coverage, j, phrase_count)

                            # Recombination
                            recombination_key = (lm_state, new_coverage)
                            if recombination_key not in stacks[len(new_coverage)] or stacks[len(new_coverage)][recombination_key].logprob < logprob:
                                stacks[len(new_coverage)][recombination_key] = new_hypothesis

    # Find the best hypothesis covering the entire sentence
    final_stack = stacks[len(f)]
    if final_stack:
        winner = max(final_stack.values(), key=lambda h: h.logprob + lm.end(h.lm_state))
        winner.logprob += lm.end(winner.lm_state)  # Add end sentence probability
    else:
        # If no valid hypotheses, create one that translates the entire sentence as-is
        winner = Hypothesis(float('-inf'), lm.begin(), None, models.phrase(" ".join(f), 0.0), set(range(len(f))), len(f), defaultdict(int))
        unaligned_count += 1

    # Extract the translation
    def extract_english(h):
        return "" if h.predecessor is None else f"{extract_english(h.predecessor)}{h.phrase.english} "

    print(extract_english(winner).strip())

    if opts.verbose:
        def extract_tm_logprob(h):
            return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
        tm_logprob = extract_tm_logprob(winner)
        sys.stderr.write(f"LM = {winner.logprob - tm_logprob}, TM = {tm_logprob}, Total = {winner.logprob}\n")

# Report unaligned sentences
if unaligned_count > 0:
    sys.stderr.write(f"ERROR: There were {unaligned_count} unaligned sentences! Only sentences that align under the model can be graded!\n")
else:
    sys.stderr.write("All sentences aligned successfully!\n")
