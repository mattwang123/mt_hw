#!/usr/bin/env python
import optparse
import sys
import models
from collections import defaultdict
from pulp import LpProblem, LpVariable, LpBinary, LpMinimize, lpSum, LpStatus, value

optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="data/input", help="File containing sentences to translate (default=data/input)")
optparser.add_option("-t", "--translation-model", dest="tm", default="data/tm", help="File containing translation model (default=data/tm)")
optparser.add_option("-l", "--language-model", dest="lm", default="data/lm", help="File containing ARPA-format language model (default=data/lm)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxsize, type=int, help="Number of sentences to decode (default=no limit)")
optparser.add_option("-k", "--translations-per-phrase", dest="k", default=5, type=int, help="Limit on number of translations to consider per phrase (default=5)")
optparser.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False, help="Verbose mode (default=off)")
opts = optparser.parse_args()[0]

tm = models.TM(opts.tm, opts.k)
lm = models.LM(opts.lm)
french_sentences = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

# Translate unknown words as-is with probability 1
for word in set(sum(french_sentences, ())):
    if (word,) not in tm:
        tm[(word,)] = [models.phrase(word, 0.0)]

sys.stderr.write(f"Decoding {opts.input}...\n")
unaligned_count = 0  # Track unaligned sentences

for idx, f in enumerate(french_sentences):
    sys.stderr.write(f"Decoding sentence {idx + 1}/{len(french_sentences)}...\n")
    # Create the ILP problem
    prob = LpProblem("MT_Decoding", LpMinimize)

    # Variables
    # x[i][j][e]: binary variable indicating if English word e is used to translate French words from position i to j
    x = defaultdict(lambda: defaultdict(dict))
    phrase_pairs = []

    # Collect all possible phrase pairs and initialize variables
    for i in range(len(f)):
        for j in range(i + 1, len(f) + 1):
            french_phrase = f[i:j]
            if french_phrase in tm:
                for phrase in tm[french_phrase]:
                    e_phrase = phrase.english
                    x[i][j][e_phrase] = LpVariable(f"x_{i}_{j}_{e_phrase}", cat=LpBinary)
                    phrase_pairs.append((i, j, e_phrase, phrase.logprob))

    # Objective Function: Minimize negative log probabilities
    prob += lpSum([-logprob * x[i][j][e_phrase] for i, j, e_phrase, logprob in phrase_pairs])

    # Constraints
    # Ensure that each position in the French sentence is covered exactly once
    for pos in range(len(f)):
        prob += lpSum([x[i][j][e_phrase]
                       for i, j, e_phrase, _ in phrase_pairs
                       if i <= pos < j]) == 1, f"Cover_{pos}"

    # Subtour elimination constraints would be added here if necessary
    # For phrase-based decoding, since phrases don't overlap and we enforce full coverage,
    # subtours are inherently prevented.

    # Solve the ILP
    prob.solve()

    # Check if the problem has an optimal solution
    if LpStatus[prob.status] != 'Optimal':
        sys.stderr.write(f"ERROR: Sentence '{' '.join(f)}' could not be aligned optimally!\n")
        winner_translation = " ".join(f)
        unaligned_count += 1
    else:
        # Extract the translation
        # Collect selected phrases
        selected_phrases = []
        for i, j, e_phrase, _ in phrase_pairs:
            if value(x[i][j][e_phrase]) == 1.0:
                selected_phrases.append((i, j, e_phrase))

        # Sort phrases by their positions
        selected_phrases.sort(key=lambda p: p[0])

        # Build the English translation
        winner_translation = ""
        for i, j, e_phrase in selected_phrases:
            winner_translation += e_phrase + " "

    print(winner_translation.strip())

    if opts.verbose:
        total_logprob = value(prob.objective)
        sys.stderr.write(f"Total logprob = {-total_logprob}\n")

# Report unaligned sentences
if unaligned_count > 0:
    sys.stderr.write(f"ERROR: There were {unaligned_count} unaligned sentences! Only sentences that align under the model can be graded!\n")
else:
    sys.stderr.write("All sentences aligned successfully!\n")
