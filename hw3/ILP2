#!/usr/bin/env python
import optparse
import sys
import models
from collections import defaultdict
from pulp import LpProblem, LpVariable, LpBinary, LpMinimize, lpSum, LpStatus, value

optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="data/input",
                     help="File containing sentences to translate (default=data/input)")
optparser.add_option("-t", "--translation-model", dest="tm", default="data/tm",
                     help="File containing translation model (default=data/tm)")
optparser.add_option("-l", "--language-model", dest="lm", default="data/lm",
                     help="File containing ARPA-format language model (default=data/lm)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxsize, type=int,
                     help="Number of sentences to decode (default=no limit)")
optparser.add_option("-k", "--translations-per-phrase", dest="k", default=15, type=int,
                     help="Limit on number of translations to consider per phrase (default=5)")
optparser.add_option("-d", "--distortion-limit", dest="dist_limit", default=1, type=int,
                     help="Maximum allowed distortion (default=3)")
optparser.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False,
                     help="Verbose mode (default=off)")
opts = optparser.parse_args()[0]

tm = models.TM(opts.tm, opts.k)
lm = models.LM(opts.lm)
french_sentences = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

# Translate unknown words as-is with probability 1
for word in set(sum(french_sentences, ())):
    if (word,) not in tm:
        tm[(word,)] = [models.phrase(word, 0.0)]

sys.stderr.write(f"Decoding {opts.input}...\n")
unaligned_count = 0  # Track unaligned sentences

for idx, f in enumerate(french_sentences):
    sys.stderr.write(f"Decoding sentence {idx + 1}/{len(french_sentences)}...\n")
    # Create the ILP problem
    prob = LpProblem("MT_Decoding", LpMinimize)

    # Variables
    # x[i][j][e_phrase][p]: binary variable indicating if English phrase e_phrase is used to translate French words from position i to j at position p in the English sentence
    x = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))
    phrase_pairs = []

    # Collect all possible phrase pairs and initialize variables
    for i in range(len(f)):
        for j in range(i + 1, len(f) + 1):
            french_phrase = f[i:j]
            if french_phrase in tm:
                for phrase in tm[french_phrase]:
                    e_phrase = phrase.english
                    # Possible positions p in the English sentence based on distortion limit
                    min_p = max(0, i - opts.dist_limit)
                    max_p = min(len(f) - 1, i + opts.dist_limit)
                    for p in range(min_p, max_p + 1):
                        var_name = f"x_{i}_{j}_{e_phrase}_{p}"
                        x[i][j][e_phrase][p] = LpVariable(var_name, cat=LpBinary)
                        phrase_pairs.append((i, j, e_phrase, p, phrase.logprob))

    # Objective Function: Minimize negative log probabilities
    prob += lpSum([-logprob * x[i][j][e_phrase][p] for i, j, e_phrase, p, logprob in phrase_pairs])

    # Constraints
    # Ensure that each position in the French sentence is covered exactly once
    for pos in range(len(f)):
        prob += lpSum([x[i][j][e_phrase][p_var]
                       for i, j, e_phrase, p_var, _ in phrase_pairs
                       if i <= pos < j]) == 1, f"Cover_{pos}"

    # Ensure that each position p in the English sentence is assigned at most one phrase
    for p in range(len(f)):
        prob += lpSum([x[i][j][e_phrase][p_var]
                       for i, j, e_phrase, p_var, _ in phrase_pairs
                       if p_var == p]) <= 1, f"EnglishPos_{p}"

    # Solve the ILP
    prob.solve()

    # Check if the problem has an optimal solution
    if LpStatus[prob.status] != 'Optimal':
        sys.stderr.write(f"ERROR: Sentence '{' '.join(f)}' could not be aligned optimally!\n")
        winner_translation = " ".join(f)
        unaligned_count += 1
    else:
        # Extract the translation
        # Collect selected phrases
        selected_phrases = []
        for i, j, e_phrase, p, _ in phrase_pairs:
            if value(x[i][j][e_phrase][p]) == 1.0:
                selected_phrases.append((p, e_phrase))

        # Sort phrases by their positions in the English sentence
        selected_phrases.sort(key=lambda p: p[0])

        # Build the English translation
        winner_translation = ""
        for p, e_phrase in selected_phrases:
            winner_translation += e_phrase + " "

    print(winner_translation.strip())

    if opts.verbose:
        total_logprob = value(prob.objective)
        sys.stderr.write(f"Total logprob = {-total_logprob}\n")

# Report unaligned sentences
if unaligned_count > 0:
    sys.stderr.write(f"ERROR: There were {unaligned_count} unaligned sentences! Only sentences that align under the model can be graded!\n")
else:
    sys.stderr.write("All sentences aligned successfully!\n")
