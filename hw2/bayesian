#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict

# Parse command-line options
optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=100000000000, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()

f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

# Initialize counts and data
f_count = defaultdict(int)
e_count = defaultdict(int)
fe_count = defaultdict(int)

# Read and preprocess the data
sys.stderr.write("Initializing counts...")
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]
for (n, (f, e)) in enumerate(bitext):
    for f_i in set(f):
        f_count[f_i] += 1
        for e_j in set(e):
            fe_count[(f_i, e_j)] += 1
    for e_j in set(e):
        e_count[e_j] += 1
    if n % 500 == 0:
        sys.stderr.write(".")

# Initialize probabilities with Bayesian smoothing
alpha = 0.01  # Smoothing parameter to handle low-frequency words
prob = defaultdict(lambda: alpha / len(e_count))  # Uniform smoothing initialization

# EM algorithm
max_iter = 10
for iteration in range(max_iter):
    sys.stderr.write(f"\nIteration {iteration + 1}/{max_iter}...\n")

    # E-step: Compute expected counts
    count = defaultdict(int)
    total_count = defaultdict(int)
    for (n, (f, e)) in enumerate(bitext):
        for i, f_i in enumerate(f):
            Z = alpha  # Add smoothing to the normalizer
            for j, e_j in enumerate(e):
                Z += prob[(f_i, e_j)]
            for j, e_j in enumerate(e):
                c = prob[(f_i, e_j)] / Z
                count[(f_i, e_j)] += c
                total_count[e_j] += c

    # M-step: Update probabilities with Bayesian smoothing
    for (f_i, e_j), cnt in count.items():
        prob[(f_i, e_j)] = (cnt + alpha) / (total_count[e_j] + alpha * len(e_count))

# Output alignments based on final probabilities
sys.stderr.write("\nGenerating alignments...\n")
for (f, e) in bitext:
    for i, f_i in enumerate(f):
        # Find the best alignment for each French word f_i
        best_j = max(range(len(e)), key=lambda j: prob[(f_i, e[j])])
        sys.stdout.write(f"{i}-{best_j} ")  # Output the alignment
    sys.stdout.write("\n")
