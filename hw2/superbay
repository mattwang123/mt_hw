#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict

# Parse command-line options
optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=100000000000, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()

f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

# Initialize counts and data
f_count = defaultdict(int)
e_count = defaultdict(int)
fe_count = defaultdict(int)

# Read and preprocess the data
sys.stderr.write("Initializing counts...\n")
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]
for (n, (f, e)) in enumerate(bitext):
    for f_i in set(f):
        f_count[f_i] += 1
        for e_j in set(e):
            fe_count[(f_i, e_j)] += 1
    for e_j in set(e):
        e_count[e_j] += 1
    if n % 500 == 0:
        sys.stderr.write(".")

# Initialize probabilities using frequency-based counts
alpha = 0.01  # Smoothing parameter
prob = defaultdict(lambda: alpha)  # Start with uniform smoothing

for (f_i, e_j), count in fe_count.items():
    prob[(f_i, e_j)] = (count + alpha) / (f_count[f_i] + alpha * len(e_count))

# EM algorithm with convergence criterion
max_iter = 10
epsilon = 1e-6  # Convergence threshold
converged = False

for iteration in range(max_iter):
    sys.stderr.write(f"\nIteration {iteration + 1}/{max_iter}...\n")

    # E-step: Compute expected counts
    count = defaultdict(int)
    total_count = defaultdict(int)
    change = 0.0  # Track change for convergence

    for (f, e) in bitext:
        for i, f_i in enumerate(f):
            Z = alpha  # Normalizer with smoothing
            for j, e_j in enumerate(e):
                Z += prob[(f_i, e_j)]
            for j, e_j in enumerate(e):
                c = prob[(f_i, e_j)] / Z
                count[(f_i, e_j)] += c
                total_count[e_j] += c

    # M-step: Update probabilities with adaptive smoothing
    for (f_i, e_j), cnt in count.items():
        new_prob = (cnt + alpha) / (total_count[e_j] + alpha * len(e_count))
        change += abs(new_prob - prob[(f_i, e_j)])
        prob[(f_i, e_j)] = new_prob

    # Check for convergence
    if change < epsilon:
        sys.stderr.write(f"Converged after {iteration + 1} iterations.\n")
        converged = True
        break

if not converged:
    sys.stderr.write("Did not converge within the maximum number of iterations.\n")

# Symmetrizing Alignments
def find_best_alignment(f_sentence, e_sentence, prob):
    alignment = []
    for i, f_i in enumerate(f_sentence):
        best_j = max(range(len(e_sentence)), key=lambda j: prob[(f_i, e_sentence[j])])
        alignment.append((i, best_j))
    return alignment

sys.stderr.write("\nGenerating alignments...\n")

f_to_e_alignments = []
e_to_f_alignments = []

# Compute French to English alignments
for f_sentence, e_sentence in bitext:
    f_to_e_alignments.append(find_best_alignment(f_sentence, e_sentence, prob))

# Compute English to French alignments
for e_sentence, f_sentence in bitext:
    e_to_f_alignments.append(find_best_alignment(e_sentence, f_sentence, prob))

# Intersection of alignments
sys.stderr.write("Symmetrizing alignments...\n")
final_alignments = []
for (f_to_e, e_to_f) in zip(f_to_e_alignments, e_to_f_alignments):
    intersection = set(f_to_e).intersection(set(e_to_f))
    final_alignments.append(intersection)


# Output final symmetrized alignments
for (f_sentence, e_sentence), alignments in zip(bitext, final_alignments):
    for i, j in alignments:
        sys.stdout.write(f"{i}-{j} ")
    sys.stdout.write("\n")
